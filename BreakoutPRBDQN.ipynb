{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 5048,
     "status": "ok",
     "timestamp": 1624520324435,
     "user": {
      "displayName": "xspmilox",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgXVa3gDnthS5GlTa_It2RnmjTaGfGUB3Ggb2kn=s64",
      "userId": "12689428043183007467"
     },
     "user_tz": -120
    },
    "id": "tqr7zRpqy2yW"
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "from src.env_wrapper import EnvWrapper\n",
    "from src.dqn_model import DQNModel\n",
    "from src.replay_buffer import PrioritizedReplayBuffer\n",
    "from src.training_data_manager import TrainingDataManager, save_models_and_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E0eSBil6y2yd"
   },
   "source": [
    "Environment Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 647,
     "status": "ok",
     "timestamp": 1624520327881,
     "user": {
      "displayName": "xspmilox",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgXVa3gDnthS5GlTa_It2RnmjTaGfGUB3Ggb2kn=s64",
      "userId": "12689428043183007467"
     },
     "user_tz": -120
    },
    "id": "mBDfJsviy2yl"
   },
   "outputs": [],
   "source": [
    "gym_env = gym.make(\"BreakoutNoFrameskip-v4\")\n",
    "env = EnvWrapper(gym_env, frame_stack=4, frame_skip=4, seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BWCFwFXuy2yr"
   },
   "source": [
    "Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bfVgTX3Hy2ys"
   },
   "outputs": [],
   "source": [
    "input_shape=(84, 84, 4, )\n",
    "n_outputs = env.action_space.n\n",
    "DQN_model = DQNModel.create(input_shape, n_outputs)\n",
    "DQN_model_target = DQNModel.create(input_shape, n_outputs)\n",
    "DQN_model_target.set_weights(DQN_model.get_weights())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zQKzSQHzy2y5"
   },
   "source": [
    "Replay Buffer Initializaion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 247,
     "status": "ok",
     "timestamp": 1624520346243,
     "user": {
      "displayName": "xspmilox",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgXVa3gDnthS5GlTa_It2RnmjTaGfGUB3Ggb2kn=s64",
      "userId": "12689428043183007467"
     },
     "user_tz": -120
    },
    "id": "CN0WIU71y2y7"
   },
   "outputs": [],
   "source": [
    "replay_buffer = PrioritizedReplayBuffer(memory_length=50000, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YtjLQrRzy2y-"
   },
   "source": [
    "Training functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 263,
     "status": "ok",
     "timestamp": 1624520350292,
     "user": {
      "displayName": "xspmilox",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgXVa3gDnthS5GlTa_It2RnmjTaGfGUB3Ggb2kn=s64",
      "userId": "12689428043183007467"
     },
     "user_tz": -120
    },
    "id": "s9sWFEqWy2y1"
   },
   "outputs": [],
   "source": [
    "discount_factor = 0.99\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.00025)\n",
    "loss_function = keras.losses.MeanSquaredError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 228,
     "status": "ok",
     "timestamp": 1624520352431,
     "user": {
      "displayName": "xspmilox",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgXVa3gDnthS5GlTa_It2RnmjTaGfGUB3Ggb2kn=s64",
      "userId": "12689428043183007467"
     },
     "user_tz": -120
    },
    "id": "rK00I0buy2zA"
   },
   "outputs": [],
   "source": [
    "def epsilon_greedy_policy(state, epsilon):\n",
    "  if np.random.rand() < epsilon:\n",
    "    return np.random.randint(n_outputs)\n",
    "  else:\n",
    "    state_tensor = tf.expand_dims(tf.convert_to_tensor(state), 0)\n",
    "    action_probs = DQN_model(state_tensor)\n",
    "    return tf.argmax(action_probs[0]).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 235,
     "status": "ok",
     "timestamp": 1624520356238,
     "user": {
      "displayName": "xspmilox",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgXVa3gDnthS5GlTa_It2RnmjTaGfGUB3Ggb2kn=s64",
      "userId": "12689428043183007467"
     },
     "user_tz": -120
    },
    "id": "0et870sXXlX1"
   },
   "outputs": [],
   "source": [
    "def training_step():\n",
    "  experiences = replay_buffer.sample_experiences()\n",
    "  states, actions, rewards, next_states, dones = experiences\n",
    "  next_Q_values = DQN_model_target(next_states)\n",
    "  max_next_Q_values = tf.reduce_max(next_Q_values, axis=1)\n",
    "  target_Q_values = rewards + (discount_factor*max_next_Q_values)*(1-dones)\n",
    "  mask = tf.one_hot(actions, n_outputs)\n",
    "  with tf.GradientTape() as tape:\n",
    "    all_Q_values = DQN_model(states)\n",
    "    Q_values = tf.reduce_sum(tf.multiply(all_Q_values, mask), axis=1)\n",
    "    loss = loss_function(target_Q_values, Q_values)\n",
    "  grads = tape.gradient(loss, DQN_model.trainable_variables)\n",
    "  optimizer.apply_gradients(zip(grads, DQN_model.trainable_variables))\n",
    "  tds = [loss_function([tq], [q]).numpy() for tq, q in zip(target_Q_values, Q_values)]\n",
    "  replay_buffer.update_prio(tds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FO7Vaju4y2zD"
   },
   "source": [
    "Data manager initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 9195,
     "status": "ok",
     "timestamp": 1624520367932,
     "user": {
      "displayName": "xspmilox",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgXVa3gDnthS5GlTa_It2RnmjTaGfGUB3Ggb2kn=s64",
      "userId": "12689428043183007467"
     },
     "user_tz": -120
    },
    "id": "tQkMJtWAy2zF"
   },
   "outputs": [],
   "source": [
    "tdm = TrainingDataManager(EnvWrapper(gym_env, frame_stack=4, frame_skip=4, seed=42), n_aav_states=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "juKXF2CFy2zH"
   },
   "source": [
    "Training Phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 296,
     "status": "ok",
     "timestamp": 1624520435026,
     "user": {
      "displayName": "xspmilox",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgXVa3gDnthS5GlTa_It2RnmjTaGfGUB3Ggb2kn=s64",
      "userId": "12689428043183007467"
     },
     "user_tz": -120
    },
    "id": "0P8Cffl9y2zI"
   },
   "outputs": [],
   "source": [
    "max_epochs = 50\n",
    "updates_per_epoch = 10000\n",
    "epsilon_max = 1.0\n",
    "epsilon_min = 0.1\n",
    "random_steps = 1e6\n",
    "target_network_update_period = 10000\n",
    "update_period = 4\n",
    "sample_period = update_period * 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 28,
     "status": "ok",
     "timestamp": 1624520436554,
     "user": {
      "displayName": "xspmilox",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgXVa3gDnthS5GlTa_It2RnmjTaGfGUB3Ggb2kn=s64",
      "userId": "12689428043183007467"
     },
     "user_tz": -120
    },
    "id": "g7nmSZNamMTx"
   },
   "outputs": [],
   "source": [
    "steps = 0\n",
    "epochs = 0\n",
    "updates = 0\n",
    "episode_reward = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cr65LsPjy2zI"
   },
   "outputs": [],
   "source": [
    "while epochs < max_epochs:\n",
    "  state = env.reset()\n",
    "  while True:\n",
    "    steps += 1\n",
    "    epsilon = max(epsilon_max - steps/random_steps, epsilon_min)\n",
    "    action = epsilon_greedy_policy(state, epsilon)\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "    episode_reward += reward\n",
    "    replay_buffer.add((state, action, reward, next_state, done))\n",
    "    state = next_state\n",
    "    if not(steps % sample_period) and replay_buffer.usable(): replay_buffer.update_sample()\n",
    "    if not(steps % update_period) and replay_buffer.usable():\n",
    "      updates += 1\n",
    "      training_step()\n",
    "      if updates >= updates_per_epoch:\n",
    "        epochs += 1\n",
    "        updates = 0\n",
    "        tdm.update_aav(DQN_model)\n",
    "        tdm.update_arpe()\n",
    "        print(f\"Epoch {epochs}/{max_epochs} concluded: reward={tdm.get_arpe()[-1]:.2f} - Îµ={epsilon:.2f} - episodes={tdm.get_episodes()}\")\n",
    "        save_models_and_data(\"adam\", \"PrioritizedDQN\", DQN_model, DQN_model_target, tdm.get_arpe(), tdm.get_aav())\n",
    "    if steps % target_network_update_period == 0:\n",
    "      DQN_model_target.set_weights(DQN_model.get_weights())\n",
    "    if done:\n",
    "      tdm.end_episode_update(episode_reward)\n",
    "      episode_reward = 0\n",
    "      break\n",
    "env.close()\n",
    "tdm.print_results()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "BreakoutPRBDQN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
